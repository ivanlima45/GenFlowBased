@InProceedings{pmlr-v97-ho19a,
  title = 	 {Flow++: Improving Flow-Based Generative Models with Variational Dequantization and Architecture Design},
  author = 	 {Ho, Jonathan and Chen, Xi and Srinivas, Aravind and Duan, Yan and Abbeel, Pieter},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {2722--2730},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
  month = 	 {09--15 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/ho19a/ho19a.pdf},
  url = 	 {http://proceedings.mlr.press/v97/ho19a.html},
  abstract = 	 {Flow-based generative models are powerful exact likelihood models with efficient sampling and inference. Despite their computational efficiency, flow-based models generally have much worse density modeling performance compared to state-of-the-art autoregressive models. In this paper, we investigate and improve upon three limiting design choices employed by flow-based models in prior work: the use of uniform noise for dequantization, the use of inexpressive affine flows, and the use of purely convolutional conditioning networks in coupling layers. Based on our findings, we propose Flow++, a new flow-based model that is now the state-of-the-art non-autoregressive model for unconditional density estimation on standard image benchmarks. Our work has begun to close the significant performance gap that has so far existed between autoregressive models and flow-based models.}
}
@incollection{NIPS2017_7181,
title = {Attention is All you Need},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {5998--6008},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf}
}

@incollection{NIPS2016_6114,
title = {Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks},
author = {Salimans, Tim and Kingma, Durk P},
booktitle = {Advances in Neural Information Processing Systems 29},
editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
pages = {901--909},
year = {2016},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/6114-weight-normalization-a-simple-reparameterization-to-accelerate-training-of-deep-neural-networks.pdf}
}
@article{DBLP:journals/corr/DinhSB16,
  author    = {Laurent Dinh and
               Jascha Sohl{-}Dickstein and
               Samy Bengio},
  title     = {Density estimation using Real {NVP}},
  journal   = {CoRR},
  volume    = {abs/1605.08803},
  year      = {2016},
  url       = {http://arxiv.org/abs/1605.08803},
  archivePrefix = {arXiv},
  eprint    = {1605.08803},
  timestamp = {Mon, 13 Aug 2018 16:47:21 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/DinhSB16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@incollection{NIPS2018_8224,
title = {Glow: Generative Flow with Invertible 1x1 Convolutions},
author = {Kingma, Durk P and Dhariwal, Prafulla},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {10215--10224},
year = {2018},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/8224-glow-generative-flow-with-invertible-1x1-convolutions.pdf}
}
